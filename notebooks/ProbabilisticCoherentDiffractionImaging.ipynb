{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKeFIyrBK+43YUQOfCsVI0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "z0l7gefLoyRw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlAVGUAIbp2t",
        "outputId": "b1687642-099b-4667-ec4e-251e124fedd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models.py\n",
        "\"\"\"\n",
        "Classes for models to be trained.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def contraction_block(in_channels: int,\n",
        "                      mid_channels: int,\n",
        "                      out_channels: int,\n",
        "                      kernel_size: int=3,\n",
        "                      stride: int=1,\n",
        "                      padding: int=1,\n",
        "                      pool_factor:int=2) -> nn.Module:\n",
        "  \"\"\"\n",
        "  Creates a constituent Conv block for the encoder section of the ptychoNN model.\n",
        "  Consists of Conv-Relu-Conv-Relu-Maxpool layers.\n",
        "  Args:\n",
        "    in_channels: Input channels to the block\n",
        "    mid_channels: intermediate channels (ie, output of first conv layer and input channels to the second)\n",
        "    out_channels: Final number of output channels from the conv block\n",
        "    kernel_size: Uniform kernel size across both conv layers in the block\n",
        "    stride: Uniform stride across both conv layers in the block\n",
        "    padding: Uniform padding across both conv layers in the block\n",
        "    pool_factor: Kernel size of the square max pool\n",
        "  Returns:\n",
        "    nn.Sequential container of modules.\n",
        "  \"\"\"\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(mid_channels),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(pool_factor)\n",
        "  )\n",
        "\n",
        "\n",
        "def expansion_block(in_channels: int,\n",
        "                    mid_channels: int,\n",
        "                    out_channels: int,\n",
        "                    kernel_size: int=3,\n",
        "                    stride: int=1,\n",
        "                    padding: int=1,\n",
        "                    upsamling_factor:int=2) -> nn.Module:\n",
        "    \"\"\"\n",
        "  Creates a constituent Conv block for the decoder sections of the ptychoNN model.\n",
        "  Consists of Conv-Relu-Conv-Relu-Upsample layers.\n",
        "  Args:\n",
        "    in_channels: Input channels to the block\n",
        "    mid_channels: intermediate channels (ie, output of first conv layer and input channels to the second)\n",
        "    out_channels: Final number of output channels from the conv block\n",
        "    kernel_size: Uniform kernel size across both conv layers in the block\n",
        "    stride: Uniform stride across both conv layers in the block\n",
        "    padding: Uniform padding across both conv layers in the block\n",
        "    upsampling_factor: Scale factor for the upsampling layer\n",
        "  Returns:\n",
        "    nn.Sequential container of modules.\n",
        "  \"\"\"\n",
        "    return nn.Sequential(\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(mid_channels),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=upsamling_factor, mode='bilinear')\n",
        "      )\n",
        "\n",
        "\n",
        "class PtychoNNBase(nn.Module):\n",
        "  \"\"\"\n",
        "  Defines the deterministic version of the PtychoNN model\n",
        "  Attributes:\n",
        "    nconv: number of feature maps from the first conv layer.\n",
        "  \"\"\"\n",
        "  def __init__(self, nconv: int=32, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = nn.Sequential(\n",
        "        contraction_block(in_channels=1, mid_channels=nconv, out_channels=nconv),\n",
        "        contraction_block(in_channels=nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        contraction_block(in_channels=2*nconv, mid_channels=4*nconv, out_channels=4*nconv)\n",
        "    )\n",
        "    self.amplitude_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.phase_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    amps = self.amplitude_decoder(encoded)\n",
        "    phis = self.phase_decoder(encoded)\n",
        "    phis = phis * np.pi\n",
        "    return amps, phis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PtychoNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Defines the deterministic version of the PtychoNN model\n",
        "  Attributes:\n",
        "    nconv: number of feature maps from the first conv layer.\n",
        "  \"\"\"\n",
        "  def __init__(self, nconv: int=32, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = nn.Sequential(\n",
        "        contraction_block(in_channels=1, mid_channels=nconv, out_channels=nconv),\n",
        "        contraction_block(in_channels=nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        contraction_block(in_channels=2*nconv, mid_channels=4*nconv, out_channels=4*nconv)\n",
        "    )\n",
        "    self.amplitude_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.phase_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    amps = self.amplitude_decoder(encoded)\n",
        "    phis = self.phase_decoder(encoded)\n",
        "    phis = phis * np.pi\n",
        "    return amps, phis\n",
        "\n",
        "  def train_step(self, ft_images, amps, phis):\n",
        "    pred_amps, pred_phis = self(ft_images)\n",
        "    amp_loss = F.mse_loss(pred_amps, amps)\n",
        "    phi_loss = F.mse_loss(pred_phis, phis)\n",
        "    amp_metric = F.l1_loss(pred_amps, amps)\n",
        "    phi_metric = F.l1_loss(pred_phis, phis)\n",
        "\n",
        "    return amp_loss, phi_loss, amp_metric, phi_metric\n",
        "\n",
        "  def eval_step(self, ft_images, amps, phis):\n",
        "    pred_amps, pred_phis = self(ft_images)\n",
        "    amp_loss = F.mse_loss(pred_amps, amps)\n",
        "    phi_loss = F.mse_loss(pred_phis, phis)\n",
        "    amp_metric = F.l1_loss(pred_amps, amps)\n",
        "    phi_metric = F.l1_loss(pred_phis, phis)\n",
        "\n",
        "    return amp_loss, phi_loss, amp_metric, phi_metric\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PtychoPNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Defines the Probabilistic Neural Network avatar of the PtychoNN model,\n",
        "  accounting for epistemic uncertainty in predictions.\n",
        "  The loss function is a NLL loss and the metric is an MSE.\n",
        "  Attributes:\n",
        "    nconv: number of feature maps from the first conv layer.\n",
        "  \"\"\"\n",
        "  def __init__(self, nconv: int=32, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = nn.Sequential(\n",
        "        contraction_block(in_channels=1, mid_channels=nconv, out_channels=nconv),\n",
        "        contraction_block(in_channels=nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        contraction_block(in_channels=2*nconv, mid_channels=4*nconv, out_channels=4*nconv)\n",
        "    )\n",
        "    self.amplitude_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "    )\n",
        "    self.amplitude_mean_end = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.amplitude_log_sigma = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
        "    )\n",
        "\n",
        "    self.phase_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "    )\n",
        "    self.phase_mean_end = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    self.phase_log_sigma = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    amps_decoded = self.amplitude_decoder(encoded)\n",
        "    amps_mean = self.amplitude_mean_end(amps_decoded)\n",
        "    amps_logsigma = self.amplitude_log_sigma(amps_decoded)\n",
        "    phis_decoded = self.phase_decoder(encoded)\n",
        "    phis_mean = self.phase_mean_end(phis_decoded)\n",
        "    phis_logsigma = self.phase_log_sigma(phis_decoded)\n",
        "    phis_mean = phis_mean * np.pi\n",
        "    return amps_mean, amps_logsigma, phis_mean, phis_logsigma\n",
        "\n",
        "  def train_step(self, ft_images, amps, phis):\n",
        "    amps_mean, amps_logsigma, phis_mean, phis_logsigma = self(ft_images)\n",
        "\n",
        "    amp_loss = F.gaussian_nll_loss(amps_mean, amps, amps_logsigma.exp().square()) #input, target, var\n",
        "    phi_loss = F.gaussian_nll_loss(phis_mean, phis, phis_logsigma.exp().square())\n",
        "    amp_metric = F.l1_loss(amps_mean, amps)\n",
        "    phi_metric = F.l1_loss(phis_mean, phis)\n",
        "\n",
        "    return amp_loss, phi_loss, amp_metric, phi_metric\n",
        "\n",
        "  def eval_step(self, ft_images, amps, phis):\n",
        "    amps_mean, amps_logsigma, phis_mean, phis_logsigma = self(ft_images)\n",
        "    amp_loss = F.gaussian_nll_loss(amps_mean, amps, amps_logsigma.exp().square()) #input, target, var\n",
        "    phi_loss = F.gaussian_nll_loss(phis_mean, phis, phis_logsigma.exp().square())\n",
        "    amp_metric = F.l1_loss(amps_mean, amps)\n",
        "    phi_metric = F.l1_loss(phis_mean, phis)\n",
        "\n",
        "    return amp_loss, phi_loss, amp_metric, phi_metric\n",
        "\n",
        "\n",
        "class DeepEnsemble(torch.nn.Module):\n",
        "  def __init__(self, models: list):\n",
        "    super().__init__()\n",
        "    self.models = torch.nn.ModuleList(models)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Treats the ensemble predictions as a Mixture of Gaussians. Computes the\n",
        "    resultant means and variances accordingly\n",
        "    \"\"\"\n",
        "    amp_mean_preds, amps_var_preds, phi_mean_preds, phi_var_preds = [], [], [], []\n",
        "    for model in self.models:\n",
        "      amps_mean, amps_logsigma, phis_mean, phis_logsigma = model(x)\n",
        "      amps_var, phis_var = amps_logsigma.exp().square(), phis_logsigma.exp().square()\n",
        "      amp_mean_preds.append(amps_mean)\n",
        "      amps_var_preds.append(amps_var)\n",
        "      phi_mean_preds.append(phis_mean)\n",
        "      phi_var_preds.append(phis_var)\n",
        "\n",
        "    amps_mean_preds = torch.stack(amps_mean_preds) #Shape: [M, 64, 1, 64,, 64]\n",
        "    amps_var_preds = torch.stack(amps_var_preds)\n",
        "    phi_mean_preds = torch.stack(phi_mean_preds)\n",
        "    phi_var_preds = torch.stack(phi_var_preds)\n",
        "    amps_ens_mean = torch.mean(amps_mean_preds, dim=0) #Shape: [64, 1, 64, 64]\n",
        "    phi_ens_mean = torch.mean(phi_mean_preds, dim=0)\n",
        "    amps_ens_var = torch.mean(amps_var_preds + amps_mean_preds.square(), dim=0) - amps_ens_mean.square()\n",
        "    phi_ens_var = torch.mean(phi_var_preds + phi_mean_preds.square(), dim=0) - phi_ens_mean.square()\n",
        "    return amps_ens_mean, amps_ens_var, phi_ens_mean, phi_ens_var\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfAsTlX-5RNk",
        "outputId": "a590c05a-bcd2-4fe3-a10e-b4ed1d33dd62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_setup.py\n",
        "\"\"\"\n",
        "Classes & Functions to download data, create datasets and dataloaders.\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
        "\n",
        "from skimage.transform import resize\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def process_data(diffraction_data: str, real_data: str, NLINES: int, H: int, W: int) -> tuple:\n",
        "  \"\"\"\n",
        "  Takes links for the diffraction and real data files, processes them and\n",
        "  returns numpy arrays for diffraction data (input) and outputs of real-space\n",
        "  amplitude and phase.\n",
        "  Args:\n",
        "    diffraction_data: path to diffraction data file\n",
        "    real_data: path to real space data file\n",
        "    NLINES: Number of lines of scanned data to use for train set\n",
        "    H: Height of images\n",
        "    W: Width of images\n",
        "  Returns:\n",
        "    tuple of (X_train, Y_I_train, Y_phi_train, X_test, Y_I_test, Y_phi_test)\n",
        "  \"\"\"\n",
        "  data_diffr = np.load(diffraction_data)['arr_0']\n",
        "  real_space = np.load(real_data)\n",
        "  amp, ph = np.abs(real_space), np.angle(real_space)\n",
        "\n",
        "  data_diffr_red = np.zeros((data_diffr.shape[0], data_diffr.shape[1], 64, 64), float)\n",
        "  for i in range(data_diffr.shape[0]):\n",
        "    for j in range(data_diffr.shape[1]):\n",
        "      data_diffr_red[i,j] = resize(data_diffr[i,j,32:-32,32:-32],(64,64),preserve_range=True, anti_aliasing=True)\n",
        "      data_diffr_red[i,j] = np.where(data_diffr_red[i,j]<3,0,data_diffr_red[i,j])\n",
        "\n",
        "  tst_strt = amp.shape[0]-NLTEST #Where to index from\n",
        "  X_train = data_diffr_red[:NLINES,:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  X_test = data_diffr_red[tst_strt:,tst_strt:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  Y_I_train = amp[:NLINES,:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  Y_I_test = amp[tst_strt:,tst_strt:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  Y_phi_train = ph[:NLINES,:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  Y_phi_test = ph[tst_strt:,tst_strt:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "\n",
        "  X_train, Y_I_train, Y_phi_train = shuffle(X_train, Y_I_train, Y_phi_train, random_state=0)\n",
        "  return X_train, Y_I_train, Y_phi_train, X_test, Y_I_test, Y_phi_test\n",
        "\n",
        "\n",
        "def get_dataloaders(data_path, val_num: int=805, batch_size: int=64, num_workers: int=2)->dict:\n",
        "  \"\"\"\n",
        "  Generates train, validation and test dataloaders from the raw numpy files.\n",
        "  Args:\n",
        "    data_path: Location of raw numpy files\n",
        "    val_num: Number of lines of scan to be saved as validation data\n",
        "    batch_size: uniform batch size\n",
        "    num_workers: number of data loader worker processes\n",
        "  Returns:\n",
        "    Dict of {\"train_dl\": train_dl, \"val_dl\": val_dl, \"test_dl\": test_dl}\n",
        "  \"\"\"\n",
        "  X_train, Y_I_train, Y_phi_train = torch.from_numpy(np.load(data_path+\"X_train.npy\")).to(torch.float), torch.from_numpy(np.load(data_path+\"Y_I_train.npy\")), torch.from_numpy(np.load(data_path+\"Y_phi_train.npy\"))\n",
        "  X_test, Y_I_test, Y_phi_test = torch.from_numpy(np.load(data_path+\"X_test.npy\")).to(torch.float), torch.from_numpy(np.load(data_path+\"Y_I_test.npy\")), torch.from_numpy(np.load(data_path+\"Y_phi_test.npy\"))\n",
        "  train_data_init = TensorDataset(X_train, Y_I_train, Y_phi_train)\n",
        "  test_dataset = TensorDataset(X_test, Y_I_test, Y_phi_test)\n",
        "  train_dataset, val_dataset = random_split(train_data_init, [X_train.shape[0]-val_num, val_num])\n",
        "\n",
        "  train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "  val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "  test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "  return {\"train_dl\": train_dl, \"val_dl\": val_dl, \"test_dl\": test_dl}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NeIsnXfBxH7",
        "outputId": "95d3a49b-7005-4998-f123-49b3baba4289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\"\"\"\n",
        "Utility functions for model training and evaluation.\n",
        "\"\"\"\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "def get_devices() -> torch.device:\n",
        "  \"\"\"\n",
        "  Returns gpu device if available, else cpu\n",
        "  \"\"\"\n",
        "  return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "def set_seeds(seed: int = 42):\n",
        "  \"\"\"\n",
        "  Sets torch seeds to ensure reproducability.\n",
        "  \"\"\"\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "\n",
        "def save_model(model_dir: str, model_name: str, model: torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Saves pytorch model in model_dir with model_name.\n",
        "  Args:\n",
        "    model_dir: Directory to save model in.\n",
        "    model_name: name of file to store model.\n",
        "    model: model to be saved.\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  os.makedirs(model_dir, exist_ok=True)\n",
        "  if not model_name.endswith(\"pt\"):\n",
        "    model_name += \".pt\"\n",
        "  torch.save(model.state_dict(), os.path.join(model_dir, model_name))\n",
        "\n",
        "\n",
        "def create_summary_writer(experiment_name: str, model_name: str, extras: str = None):\n",
        "        # -> torch.utils.tensorboard.SummaryWriter:\n",
        "  \"\"\"\n",
        "  Instantiates and returns a Summary writer for the experiment, that writers to\n",
        "  runs/experiment_name/model_name/extras\n",
        "  Args:\n",
        "    experiment_name: Name of experiment (say, dataset)\n",
        "    model_name: Name of model used\n",
        "    extras: Additional details\n",
        "  Returns:\n",
        "    SummaryWriter instance for the experiment\n",
        "  \"\"\"\n",
        "  if extras:\n",
        "    log_dir = os.path.join(\"runs/\", experiment_name, model_name, extras)\n",
        "  else:\n",
        "    log_dir = os.path.join(\"runs/\", experiment_name, model_name)\n",
        "  writer = torch.utils.tensorboard.SummaryWriter(log_dir)\n",
        "  return writer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D0vEOhkPU8g",
        "outputId": "9e445ee9-12ef-4927-f7e0-5894203d53c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile engine.py\n",
        "\"\"\"\n",
        "Functions to train and evaluate model on the image dataset\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               train_dl: torch.utils.data.DataLoader,\n",
        "               opt: torch.optim.Optimizer,\n",
        "               device: torch.device\n",
        "               ) -> dict:\n",
        "  \"\"\"\n",
        "  Performs 1 epoch of training of model on train dataloader,\n",
        "  returning model loss on the amplitude and phase reconstruction.\n",
        "  Args:\n",
        "    model: model too be trained\n",
        "    train_dl: Dataloader with training data\n",
        "    loss_fn: Differentiable loss function to be used for gradients\n",
        "    opt: Optimizer to train model.\n",
        "    device: Device on which model and data will reside.\n",
        "  Returns:\n",
        "      Dict with keys \"amp_loss\", \"phase_loss\", \"amp_metric\", \"phase_metric\".\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  amplitude_loss, phase_loss, amplitude_metric, phase_metric = 0.0, 0.0, 0.0, 0.0\n",
        "  for ft_images, amps, phis in train_dl:\n",
        "    ft_images, amps, phis = ft_images.to(device), amps.to(device), phis.to(device)\n",
        "    # pred_amps, pred_phis = model(ft_images)\n",
        "    # amp_loss = loss_fn(pred_amps, amps)\n",
        "    # phi_loss = loss_fn(pred_phis, phis)\n",
        "    amp_loss, phi_loss, amp_metric, phi_metric = model.train_step(ft_images, amps, phis)\n",
        "    loss = amp_loss + phi_loss\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    amplitude_loss += amp_loss.detach().item()\n",
        "    phase_loss += phi_loss.detach().item()\n",
        "    amplitude_metric += amp_metric.detach().item()\n",
        "    phase_metric += phi_metric.detach().item()\n",
        "\n",
        "  model.eval()\n",
        "  return {\"amp_loss\": amplitude_loss/len(train_dl),\n",
        "          \"phase_loss\": phase_loss/len(train_dl),\n",
        "          \"amp_metric\": amplitude_metric/len(train_dl),\n",
        "          \"phase_metric\": phase_metric/len(train_dl)}\n",
        "\n",
        "\n",
        "def val_step(model: torch.nn.Module,\n",
        "            val_dl: torch.utils.data.DataLoader,\n",
        "            device: torch.device\n",
        "            ) -> dict:\n",
        "  \"\"\"\n",
        "  Performs 1 epoch of evaluation of model on validation dataloader,\n",
        "  returning model loss on the amplitude and phase reconstruction.\n",
        "  Args:\n",
        "    model: model too be trained\n",
        "    train_dl: Dataloader with training data\n",
        "    loss_fn: Loss function to be used for gradients\n",
        "    device: Device on which model and data will reside.\n",
        "  Returns:\n",
        "      Dict with keys \"total_loss\", \"amp_loss\" and \"phase_loss\".\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  amplitude_loss, phase_loss, amplitude_metric, phase_metric = 0.0, 0.0, 0.0, 0.0\n",
        "  with torch.inference_mode():\n",
        "    for ft_images, amps, phis in val_dl:\n",
        "      ft_images, amps, phis = ft_images.to(device), amps.to(device), phis.to(device)\n",
        "      # pred_amps, pred_phis = model(ft_images)\n",
        "      # amp_loss = loss_fn(pred_amps, amps)\n",
        "      # phi_loss = loss_fn(pred_phis, phis)\n",
        "      amp_loss, phi_loss, amp_metric, phi_metric = model.eval_step(ft_images, amps, phis)\n",
        "      loss = amp_loss + phi_loss\n",
        "\n",
        "      amplitude_loss += amp_loss.detach().item()\n",
        "      phase_loss += phi_loss.detach().item()\n",
        "      amplitude_metric += amp_metric.detach().item()\n",
        "      phase_metric += phi_metric.detach().item()\n",
        "\n",
        "  return {\"amp_loss\": amplitude_loss/len(val_dl),\n",
        "          \"phase_loss\": phase_loss/len(val_dl),\n",
        "          \"amp_metric\": amplitude_metric/len(val_dl),\n",
        "          \"phase_metric\": phase_metric/len(val_dl)}\n",
        "\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dl: torch.utils.data.DataLoader,\n",
        "          val_dl: torch.utils.data.DataLoader,\n",
        "          opt: torch.optim.Optimizer,\n",
        "          device: torch.device,\n",
        "          num_epochs: int) -> dict:\n",
        "  \"\"\"\n",
        "  Performs defined number of epochs of training and evaluation for the model on\n",
        "  the data loaders, returning the loss history on amplitude and phase reconstruction.\n",
        "  Args:\n",
        "    model: model to be trained and evaluated.\n",
        "    train_dl: Dataloader with training data.\n",
        "    val_dl: Dataloader with testing data.\n",
        "    loss_fn: Differentiable loss function to use for gradients.\n",
        "    opt: Optimizer to tune model params.\n",
        "    device: Device on which model and eventually data shall reside\n",
        "    num_epochs: Number of epochs of training\n",
        "  Returns:\n",
        "    Dict with history of \"total_loss\", \"amp_loss\" and \"phase_loss\".\n",
        "  \"\"\"\n",
        "  amp_loss_train, phi_loss_train, amp_loss_val, phi_loss_val = [], [], [], []\n",
        "  amp_metric_train, phi_metric_train, amp_metric_val, phi_metric_val = [], [], [], []\n",
        "  for epoch in range(num_epochs):\n",
        "    train_results = train_step(model, train_dl, opt, device)\n",
        "    val_results = val_step(model, val_dl, device)\n",
        "    amp_loss_train.append(train_results[\"amp_loss\"])\n",
        "    phi_loss_train.append(train_results[\"phase_loss\"])\n",
        "    amp_loss_val.append(val_results[\"amp_loss\"])\n",
        "    phi_loss_val.append(val_results[\"phase_loss\"])\n",
        "    amp_metric_train.append(train_results[\"amp_metric\"])\n",
        "    phi_metric_train.append(train_results[\"phase_metric\"])\n",
        "    amp_metric_val.append(val_results[\"amp_metric\"])\n",
        "    phi_metric_val.append(val_results[\"phase_metric\"])\n",
        "    print(f\"Epoch: {epoch+1} Train Amp Loss: {amp_loss_train[-1]:.5f} Train Phi Loss: {phi_loss_train[-1]:.5f} Val Amp Loss: {amp_loss_val[-1]:.5f} Val Phi Loss: {phi_loss_val[-1]:.5f}\")\n",
        "    print(f\"Epoch: {epoch+1} Train Amp Metric: {amp_metric_train[-1]:.5f} Train Phi Metric: {phi_metric_train[-1]:.5f} Val Amp Metric: {amp_metric_val[-1]:.5f} Val Phi Metric: {phi_metric_val[-1]:.5f}\")\n",
        "\n",
        "  return {\"amp_loss_train\": amp_loss_train, \"phi_loss_train\": phi_loss_train,\n",
        "          \"amp_loss_val\": amp_loss_val, \"phi_loss_val\": phi_loss_val,\n",
        "          \"amp_metric_train\": amp_metric_train, \"phi_metric_train\": phi_metric_train,\n",
        "          \"amp_metric_val\": amp_metric_val, \"phi_metric_val\": phi_metric_val}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOwUkrB-qzfx",
        "outputId": "0db68fe5-74b4-4f2f-9296-248e408e3fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\"\"\"\n",
        "Takes parameters from user; trains, evaluates and saves models on\n",
        "Coherent Diffraction Imaging Data.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "from data_setup import get_dataloaders\n",
        "from models import PtychoNN, PtychoPNN\n",
        "from engine import train\n",
        "from utils import get_devices, set_seeds, save_model\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--data_path\", type=str, default=\"./gdrive/MyDrive/PtychoNNData/\")\n",
        "parser.add_argument(\"--num_epochs\", type=int, default=75)\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "parser.add_argument(\"--model\", type=str, default=\"PtychoNN\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "set_seeds(42)\n",
        "device = get_devices()\n",
        "d = get_dataloaders(args.data_path)\n",
        "train_dl, val_dl, test_dl = d[\"train_dl\"], d[\"val_dl\"], d[\"test_dl\"]\n",
        "model = PtychoPNN().to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "results = train(model, train_dl, val_dl, opt, device, args.num_epochs)\n",
        "model_name = args.model + str(args.num_epochs)\n",
        "save_model(\"./Models\", \"model_\" + args.model, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTJ7HT_D9_ow",
        "outputId": "d934acd3-284a-42d7-f3b6-c5d4d8eb08ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python train.py"
      ],
      "metadata": {
        "id": "D9UA2HV56Apc"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}