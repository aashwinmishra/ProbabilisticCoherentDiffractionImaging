{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqYef9xz7SU0/8/o+acFFJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
        "\n",
        "from skimage.transform import resize\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "z0l7gefLoyRw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlAVGUAIbp2t",
        "outputId": "7cdbd14a-99f2-46eb-f8e0-3afaddef0011"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models.py\n",
        "\"\"\"\n",
        "Classes for models to be trained.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def contraction_block(in_channels: int,\n",
        "                      mid_channels: int,\n",
        "                      out_channels: int,\n",
        "                      kernel_size: int=3,\n",
        "                      stride: int=1,\n",
        "                      padding: int=1,\n",
        "                      pool_factor:int=2) -> nn.Module:\n",
        "  \"\"\"\n",
        "  Creates a constituent Conv block for the encoder section of the ptychoNN model.\n",
        "  Consists of Conv-Relu-Conv-Relu-Maxpool layers.\n",
        "  Args:\n",
        "    in_channels: Input channels to the block\n",
        "    mid_channels: intermediate channels (ie, output of first conv layer and input channels to the second)\n",
        "    out_channels: Final number of output channels from the conv block\n",
        "    kernel_size: Uniform kernel size across both conv layers in the block\n",
        "    stride: Uniform stride across both conv layers in the block\n",
        "    padding: Uniform padding across both conv layers in the block\n",
        "    pool_factor: Kernel size of the square max pool\n",
        "  Returns:\n",
        "    nn.Sequential container of modules.\n",
        "  \"\"\"\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(mid_channels),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(pool_factor)\n",
        "  )\n",
        "\n",
        "\n",
        "def expansion_block(in_channels: int,\n",
        "                    mid_channels: int,\n",
        "                    out_channels: int,\n",
        "                    kernel_size: int=3,\n",
        "                    stride: int=1,\n",
        "                    padding: int=1,\n",
        "                    upsamling_factor:int=2) -> nn.Module:\n",
        "    \"\"\"\n",
        "  Creates a constituent Conv block for the decoder sections of the ptychoNN model.\n",
        "  Consists of Conv-Relu-Conv-Relu-Upsample layers.\n",
        "  Args:\n",
        "    in_channels: Input channels to the block\n",
        "    mid_channels: intermediate channels (ie, output of first conv layer and input channels to the second)\n",
        "    out_channels: Final number of output channels from the conv block\n",
        "    kernel_size: Uniform kernel size across both conv layers in the block\n",
        "    stride: Uniform stride across both conv layers in the block\n",
        "    padding: Uniform padding across both conv layers in the block\n",
        "    upsampling_factor: Scale factor for the upsampling layer\n",
        "  Returns:\n",
        "    nn.Sequential container of modules.\n",
        "  \"\"\"\n",
        "    return nn.Sequential(\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(mid_channels),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=upsamling_factor, mode='bilinear')\n",
        "      )\n",
        "\n",
        "\n",
        "class PtychoNNBase(nn.Module):\n",
        "  \"\"\"\n",
        "  Defines the deterministic version of the PtychoNN model\n",
        "  Attributes:\n",
        "    nconv: number of feature maps from the first conv layer.\n",
        "  \"\"\"\n",
        "  def __init__(self, nconv: int=32, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = nn.Sequential(\n",
        "        contraction_block(in_channels=1, mid_channels=nconv, out_channels=nconv),\n",
        "        contraction_block(in_channels=nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        contraction_block(in_channels=2*nconv, mid_channels=4*nconv, out_channels=4*nconv)\n",
        "    )\n",
        "    self.amplitude_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.phase_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    amps = self.amplitude_decoder(encoded)\n",
        "    phis = self.phase_decoder(encoded)\n",
        "    phis = phis * np.pi\n",
        "    return amps, phis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PtychoNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Defines the deterministic version of the PtychoNN model\n",
        "  Attributes:\n",
        "    nconv: number of feature maps from the first conv layer.\n",
        "  \"\"\"\n",
        "  def __init__(self, nconv: int=32, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = nn.Sequential(\n",
        "        contraction_block(in_channels=1, mid_channels=nconv, out_channels=nconv),\n",
        "        contraction_block(in_channels=nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        contraction_block(in_channels=2*nconv, mid_channels=4*nconv, out_channels=4*nconv)\n",
        "    )\n",
        "    self.amplitude_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.phase_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    amps = self.amplitude_decoder(encoded)\n",
        "    phis = self.phase_decoder(encoded)\n",
        "    phis = phis * np.pi\n",
        "    return amps, phis\n",
        "\n",
        "  def train_step(self, ft_images, amps, phis):\n",
        "    pred_amps, pred_phis = self(ft_images)\n",
        "    amp_loss = F.mse_loss(pred_amps, amps)\n",
        "    phi_loss = F.mse_loss(pred_phis, phis)\n",
        "    amp_metric = F.l1_loss(pred_amps, amps)\n",
        "    phi_metric = F.l1_loss(pred_phis, phis)\n",
        "\n",
        "    return amp_loss, phi_loss, amp_metric, phi_metric\n",
        "\n",
        "  def eval_step(self, ft_images, amps, phis):\n",
        "    pred_amps, pred_phis = self(ft_images)\n",
        "    amp_loss = F.mse_loss(pred_amps, amps)\n",
        "    phi_loss = F.mse_loss(pred_phis, phis)\n",
        "    amp_metric = F.l1_loss(pred_amps, amps)\n",
        "    phi_metric = F.l1_loss(pred_phis, phis)\n",
        "\n",
        "    return amp_loss, phi_loss, amp_metric, phi_metric\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PtychoPNN(nn.Module):\n",
        "  \"\"\"\n",
        "  Defines the Probabilistic Neural Network avatar of the PtychoNN model,\n",
        "  accounting for epistemic uncertainty in predictions.\n",
        "  The loss function is a NLL loss and the metric is an MSE.\n",
        "  Attributes:\n",
        "    nconv: number of feature maps from the first conv layer.\n",
        "  \"\"\"\n",
        "  def __init__(self, nconv: int=32, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = nn.Sequential(\n",
        "        contraction_block(in_channels=1, mid_channels=nconv, out_channels=nconv),\n",
        "        contraction_block(in_channels=nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        contraction_block(in_channels=2*nconv, mid_channels=4*nconv, out_channels=4*nconv)\n",
        "    )\n",
        "    self.amplitude_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "    )\n",
        "    self.amplitude_mean_end = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.amplitude_log_sigma = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
        "    )\n",
        "\n",
        "    self.phase_decoder = nn.Sequential(\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=4*nconv, out_channels=4*nconv),\n",
        "        expansion_block(in_channels=4*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "        expansion_block(in_channels=2*nconv, mid_channels=2*nconv, out_channels=2*nconv),\n",
        "    )\n",
        "    self.phase_mean_end = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    self.phase_log_sigma = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=2*nconv, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    amps_decoded = self.amplitude_decoder(encoded)\n",
        "    amps_mean = self.amplitude_mean_end(amps_decoded)\n",
        "    amps_logsigma = self.amplitude_log_sigma(amps_decoded)\n",
        "    phis_decoded = self.phase_decoder(encoded)\n",
        "    phis_mean = self.phase_mean_end(phis_decoded)\n",
        "    phis_logsigma = self.phase_log_sigma(phis_decoded)\n",
        "    phis_mean = phis_mean * np.pi\n",
        "    return amps_mean, amps_logsigma, phis_mean, phis_logsigma\n",
        "\n",
        "  def train_step(self, ft_images, amps, phis):\n",
        "    amps_mean, amps_logsigma, phis_mean, phis_logsigma = self(ft_images)\n",
        "\n",
        "    amp_loss = F.gaussian_nll_loss(amps_mean, amps, amps_logsigma.exp().square()) #input, target, var\n",
        "    phi_loss = F.gaussian_nll_loss(phis_mean, phis, phis_logsigma.exp().square())\n",
        "    amp_metric = F.l1_loss(amps_mean, amps)\n",
        "    phi_metric = F.l1_loss(phis_mean, phis)\n",
        "\n",
        "    return amp_loss, phi_loss, amp_metric, phi_metric\n",
        "\n",
        "  def eval_step(self, ft_images, amps, phis):\n",
        "    amps_mean, amps_logsigma, phis_mean, phis_logsigma = self(ft_images)\n",
        "    amp_loss = F.gaussian_nll_loss(amps_mean, amps, amps_logsigma.exp().square()) #input, target, var\n",
        "    phi_loss = F.gaussian_nll_loss(phis_mean, phis, phis_logsigma.exp().square())\n",
        "    amp_metric = F.l1_loss(amps_mean, amps)\n",
        "    phi_metric = F.l1_loss(phis_mean, phis)\n",
        "\n",
        "    return amp_loss, phi_loss, amp_metric, phi_metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfAsTlX-5RNk",
        "outputId": "2e9b4f96-3d8a-4ed1-97d9-8182fba09c5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# H, W = 64, 64\n",
        "# NLINES = 100\n",
        "# NLTEST = 60\n",
        "# N_VALID = 805"
      ],
      "metadata": {
        "id": "HluNpILJRXze"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_setup.py\n",
        "\"\"\"\n",
        "Classes & Functions to download data, create datasets and dataloaders.\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
        "\n",
        "from skimage.transform import resize\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def process_data(diffraction_data: str, real_data: str, NLINES: int, H: int, W: int) -> tuple:\n",
        "  \"\"\"\n",
        "  Takes links for the diffraction and real data files, processes them and\n",
        "  returns numpy arrays for diffraction data (input) and outputs of real-space\n",
        "  amplitude and phase.\n",
        "  Args:\n",
        "    diffraction_data: path to diffraction data file\n",
        "    real_data: path to real space data file\n",
        "    NLINES: Number of lines of scanned data to use for train set\n",
        "    H: Height of images\n",
        "    W: Width of images\n",
        "  Returns:\n",
        "    tuple of (X_train, Y_I_train, Y_phi_train, X_test, Y_I_test, Y_phi_test)\n",
        "  \"\"\"\n",
        "  data_diffr = np.load(diffraction_data)['arr_0']\n",
        "  real_space = np.load(real_data)\n",
        "  amp, ph = np.abs(real_space), np.angle(real_space)\n",
        "\n",
        "  data_diffr_red = np.zeros((data_diffr.shape[0], data_diffr.shape[1], 64, 64), float)\n",
        "  for i in range(data_diffr.shape[0]):\n",
        "    for j in range(data_diffr.shape[1]):\n",
        "      data_diffr_red[i,j] = resize(data_diffr[i,j,32:-32,32:-32],(64,64),preserve_range=True, anti_aliasing=True)\n",
        "      data_diffr_red[i,j] = np.where(data_diffr_red[i,j]<3,0,data_diffr_red[i,j])\n",
        "\n",
        "  tst_strt = amp.shape[0]-NLTEST #Where to index from\n",
        "  X_train = data_diffr_red[:NLINES,:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  X_test = data_diffr_red[tst_strt:,tst_strt:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  Y_I_train = amp[:NLINES,:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  Y_I_test = amp[tst_strt:,tst_strt:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  Y_phi_train = ph[:NLINES,:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "  Y_phi_test = ph[tst_strt:,tst_strt:].reshape(-1,H,W)[:,np.newaxis,:,:]\n",
        "\n",
        "  X_train, Y_I_train, Y_phi_train = shuffle(X_train, Y_I_train, Y_phi_train, random_state=0)\n",
        "  return X_train, Y_I_train, Y_phi_train, X_test, Y_I_test, Y_phi_test\n",
        "\n",
        "\n",
        "def get_dataloaders(data_path, val_num: int=805, batch_size: int=64, num_workers: int=2)->dict:\n",
        "  \"\"\"\n",
        "  Generates train, validation and test dataloaders from the raw numpy files.\n",
        "  Args:\n",
        "    data_path: Location of raw numpy files\n",
        "    val_num: Number of lines of scan to be saved as validation data\n",
        "    batch_size: uniform batch size\n",
        "    num_workers: number of data loader worker processes\n",
        "  Returns:\n",
        "    Dict of {\"train_dl\": train_dl, \"val_dl\": val_dl, \"test_dl\": test_dl}\n",
        "  \"\"\"\n",
        "  X_train, Y_I_train, Y_phi_train = torch.from_numpy(np.load(data_path+\"X_train.npy\")).to(torch.float), torch.from_numpy(np.load(data_path+\"Y_I_train.npy\")), torch.from_numpy(np.load(data_path+\"Y_phi_train.npy\"))\n",
        "  X_test, Y_I_test, Y_phi_test = torch.from_numpy(np.load(data_path+\"X_test.npy\")).to(torch.float), torch.from_numpy(np.load(data_path+\"Y_I_test.npy\")), torch.from_numpy(np.load(data_path+\"Y_phi_test.npy\"))\n",
        "  train_data_init = TensorDataset(X_train, Y_I_train, Y_phi_train)\n",
        "  test_dataset = TensorDataset(X_test, Y_I_test, Y_phi_test)\n",
        "  train_dataset, val_dataset = random_split(train_data_init, [X_train.shape[0]-val_num, val_num])\n",
        "\n",
        "  train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "  val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "  test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "  return {\"train_dl\": train_dl, \"val_dl\": val_dl, \"test_dl\": test_dl}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NeIsnXfBxH7",
        "outputId": "95d3a49b-7005-4998-f123-49b3baba4289",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\"\"\"\n",
        "Utility functions for model training and evaluation.\n",
        "\"\"\"\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "def get_devices() -> torch.device:\n",
        "  \"\"\"\n",
        "  Returns gpu device if available, else cpu\n",
        "  \"\"\"\n",
        "  return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "def set_seeds(seed: int = 42):\n",
        "  \"\"\"\n",
        "  Sets torch seeds to ensure reproducability.\n",
        "  \"\"\"\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "\n",
        "def save_model(model_dir: str, model_name: str, model: torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Saves pytorch model in model_dir with model_name.\n",
        "  Args:\n",
        "    model_dir: Directory to save model in.\n",
        "    model_name: name of file to store model.\n",
        "    model: model to be saved.\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  os.makedirs(model_dir, exist_ok=True)\n",
        "  if not model_name.endswith(\"pt\"):\n",
        "    model_name += \".pt\"\n",
        "  torch.save(model.state_dict(), os.path.join(model_dir, model_name))\n",
        "\n",
        "\n",
        "def create_summary_writer(experiment_name: str, model_name: str, extras: str = None):\n",
        "        # -> torch.utils.tensorboard.SummaryWriter:\n",
        "  \"\"\"\n",
        "  Instantiates and returns a Summary writer for the experiment, that writers to\n",
        "  runs/experiment_name/model_name/extras\n",
        "  Args:\n",
        "    experiment_name: Name of experiment (say, dataset)\n",
        "    model_name: Name of model used\n",
        "    extras: Additional details\n",
        "  Returns:\n",
        "    SummaryWriter instance for the experiment\n",
        "  \"\"\"\n",
        "  if extras:\n",
        "    log_dir = os.path.join(\"runs/\", experiment_name, model_name, extras)\n",
        "  else:\n",
        "    log_dir = os.path.join(\"runs/\", experiment_name, model_name)\n",
        "  writer = torch.utils.tensorboard.SummaryWriter(log_dir)\n",
        "  return writer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D0vEOhkPU8g",
        "outputId": "9e445ee9-12ef-4927-f7e0-5894203d53c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile engine.py\n",
        "\"\"\"\n",
        "Functions to train and evaluate model on the image dataset\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               train_dl: torch.utils.data.DataLoader,\n",
        "               opt: torch.optim.Optimizer,\n",
        "               device: torch.device\n",
        "               ) -> dict:\n",
        "  \"\"\"\n",
        "  Performs 1 epoch of training of model on train dataloader,\n",
        "  returning model loss on the amplitude and phase reconstruction.\n",
        "  Args:\n",
        "    model: model too be trained\n",
        "    train_dl: Dataloader with training data\n",
        "    loss_fn: Differentiable loss function to be used for gradients\n",
        "    opt: Optimizer to train model.\n",
        "    device: Device on which model and data will reside.\n",
        "  Returns:\n",
        "      Dict with keys \"amp_loss\", \"phase_loss\", \"amp_metric\", \"phase_metric\".\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  amplitude_loss, phase_loss, amplitude_metric, phase_metric = 0.0, 0.0, 0.0, 0.0\n",
        "  for ft_images, amps, phis in train_dl:\n",
        "    ft_images, amps, phis = ft_images.to(device), amps.to(device), phis.to(device)\n",
        "    # pred_amps, pred_phis = model(ft_images)\n",
        "    # amp_loss = loss_fn(pred_amps, amps)\n",
        "    # phi_loss = loss_fn(pred_phis, phis)\n",
        "    amp_loss, phi_loss, amp_metric, phi_metric = model.train_step(ft_images, amps, phis)\n",
        "    loss = amp_loss + phi_loss\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    amplitude_loss += amp_loss.detach().item()\n",
        "    phase_loss += phi_loss.detach().item()\n",
        "    amplitude_metric += amp_metric.detach().item()\n",
        "    phase_metric += phi_metric.detach().item()\n",
        "\n",
        "  model.eval()\n",
        "  return {\"amp_loss\": amplitude_loss/len(train_dl),\n",
        "          \"phase_loss\": phase_loss/len(train_dl),\n",
        "          \"amp_metric\": amplitude_metric/len(train_dl),\n",
        "          \"phase_metric\": phase_metric/len(train_dl)}\n",
        "\n",
        "\n",
        "def val_step(model: torch.nn.Module,\n",
        "            val_dl: torch.utils.data.DataLoader,\n",
        "            device: torch.device\n",
        "            ) -> dict:\n",
        "  \"\"\"\n",
        "  Performs 1 epoch of evaluation of model on validation dataloader,\n",
        "  returning model loss on the amplitude and phase reconstruction.\n",
        "  Args:\n",
        "    model: model too be trained\n",
        "    train_dl: Dataloader with training data\n",
        "    loss_fn: Loss function to be used for gradients\n",
        "    device: Device on which model and data will reside.\n",
        "  Returns:\n",
        "      Dict with keys \"total_loss\", \"amp_loss\" and \"phase_loss\".\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  amplitude_loss, phase_loss, amplitude_metric, phase_metric = 0.0, 0.0, 0.0, 0.0\n",
        "  with torch.inference_mode():\n",
        "    for ft_images, amps, phis in val_dl:\n",
        "      ft_images, amps, phis = ft_images.to(device), amps.to(device), phis.to(device)\n",
        "      # pred_amps, pred_phis = model(ft_images)\n",
        "      # amp_loss = loss_fn(pred_amps, amps)\n",
        "      # phi_loss = loss_fn(pred_phis, phis)\n",
        "      amp_loss, phi_loss, amp_metric, phi_metric = model.eval_step(ft_images, amps, phis)\n",
        "      loss = amp_loss + phi_loss\n",
        "\n",
        "      amplitude_loss += amp_loss.detach().item()\n",
        "      phase_loss += phi_loss.detach().item()\n",
        "      amplitude_metric += amp_metric.detach().item()\n",
        "      phase_metric += phi_metric.detach().item()\n",
        "\n",
        "  return {\"amp_loss\": amplitude_loss/len(val_dl),\n",
        "          \"phase_loss\": phase_loss/len(val_dl),\n",
        "          \"amp_metric\": amplitude_metric/len(val_dl),\n",
        "          \"phase_metric\": phase_metric/len(val_dl)}\n",
        "\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dl: torch.utils.data.DataLoader,\n",
        "          val_dl: torch.utils.data.DataLoader,\n",
        "          opt: torch.optim.Optimizer,\n",
        "          device: torch.device,\n",
        "          num_epochs: int) -> dict:\n",
        "  \"\"\"\n",
        "  Performs defined number of epochs of training and evaluation for the model on\n",
        "  the data loaders, returning the loss history on amplitude and phase reconstruction.\n",
        "  Args:\n",
        "    model: model to be trained and evaluated.\n",
        "    train_dl: Dataloader with training data.\n",
        "    val_dl: Dataloader with testing data.\n",
        "    loss_fn: Differentiable loss function to use for gradients.\n",
        "    opt: Optimizer to tune model params.\n",
        "    device: Device on which model and eventually data shall reside\n",
        "    num_epochs: Number of epochs of training\n",
        "  Returns:\n",
        "    Dict with history of \"total_loss\", \"amp_loss\" and \"phase_loss\".\n",
        "  \"\"\"\n",
        "  amp_loss_train, phi_loss_train, amp_loss_val, phi_loss_val = [], [], [], []\n",
        "  amp_metric_train, phi_metric_train, amp_metric_val, phi_metric_val = [], [], [], []\n",
        "  for epoch in range(num_epochs):\n",
        "    train_results = train_step(model, train_dl, opt, device)\n",
        "    val_results = val_step(model, val_dl, device)\n",
        "    amp_loss_train.append(train_results[\"amp_loss\"])\n",
        "    phi_loss_train.append(train_results[\"phase_loss\"])\n",
        "    amp_loss_val.append(val_results[\"amp_loss\"])\n",
        "    phi_loss_val.append(val_results[\"phase_loss\"])\n",
        "    amp_metric_train.append(train_results[\"amp_metric\"])\n",
        "    phi_metric_train.append(train_results[\"phase_metric\"])\n",
        "    amp_metric_val.append(val_results[\"amp_metric\"])\n",
        "    phi_metric_val.append(val_results[\"phase_metric\"])\n",
        "    print(f\"Epoch: {epoch+1} Train Amp Loss: {amp_loss_train[-1]:.5f} Train Phi Loss: {phi_loss_train[-1]:.5f} Val Amp Loss: {amp_loss_val[-1]:.5f} Val Phi Loss: {phi_loss_val[-1]:.5f}\")\n",
        "    print(f\"Epoch: {epoch+1} Train Amp Metric: {amp_metric_train[-1]:.5f} Train Phi Metric: {phi_metric_train[-1]:.5f} Val Amp Metric: {amp_metric_val[-1]:.5f} Val Phi Metric: {phi_metric_val[-1]:.5f}\")\n",
        "\n",
        "  return {\"amp_loss_train\": amp_loss_train, \"phi_loss_train\": phi_loss_train,\n",
        "          \"amp_loss_val\": amp_loss_val, \"phi_loss_val\": phi_loss_val,\n",
        "          \"amp_metric_train\": amp_metric_train, \"phi_metric_train\": phi_metric_train,\n",
        "          \"amp_metric_val\": amp_metric_val, \"phi_metric_val\": phi_metric_val}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOwUkrB-qzfx",
        "outputId": "0db68fe5-74b4-4f2f-9296-248e408e3fa7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\"\"\"\n",
        "Takes parameters from user; trains, evaluates and saves models on\n",
        "Coherent Diffraction Imaging Data.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "from data_setup import get_dataloaders\n",
        "from models import PtychoNN, PtychoPNN\n",
        "from engine import train\n",
        "from utils import get_devices, set_seeds, save_model\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--data_path\", type=str, default=\"./gdrive/MyDrive/PtychoNNData/\")\n",
        "parser.add_argument(\"--num_epochs\", type=int, default=75)\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "parser.add_argument(\"--model\", type=str, default=\"PtychoNN\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "set_seeds(42)\n",
        "device = get_devices()\n",
        "d = get_dataloaders(args.data_path)\n",
        "train_dl, val_dl, test_dl = d[\"train_dl\"], d[\"val_dl\"], d[\"test_dl\"]\n",
        "model = PtychoPNN().to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "results = train(model, train_dl, val_dl, opt, device, args.num_epochs)\n",
        "model_name = args.model + str(args.num_epochs)\n",
        "save_model(\"./Models\", \"model_\" + args.model, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTJ7HT_D9_ow",
        "outputId": "d934acd3-284a-42d7-f3b6-c5d4d8eb08ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9UA2HV56Apc",
        "outputId": "6641f181-784f-483d-8db6-57c0ddfb351f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Train Amp Loss: -2.61867 Train Phi Loss: 0.10528 Val Amp Loss: -3.98941 Val Phi Loss: -0.06045\n",
            "Epoch: 1 Train Amp Metric: 0.06441 Train Phi Metric: 0.59055 Val Amp Metric: 0.00975 Val Phi Metric: 0.51094\n",
            "Epoch: 2 Train Amp Loss: -4.16967 Train Phi Loss: -0.16356 Val Amp Loss: -4.24345 Val Phi Loss: -0.21500\n",
            "Epoch: 2 Train Amp Metric: 0.00798 Train Phi Metric: 0.47110 Val Amp Metric: 0.00716 Val Phi Metric: 0.45258\n",
            "Epoch: 3 Train Amp Loss: -4.29502 Train Phi Loss: -0.29608 Val Amp Loss: -4.34528 Val Phi Loss: -0.36282\n",
            "Epoch: 3 Train Amp Metric: 0.00699 Train Phi Metric: 0.42322 Val Amp Metric: 0.00679 Val Phi Metric: 0.40450\n",
            "Epoch: 4 Train Amp Loss: -4.36022 Train Phi Loss: -0.40452 Val Amp Loss: -4.35053 Val Phi Loss: -0.32901\n",
            "Epoch: 4 Train Amp Metric: 0.00672 Train Phi Metric: 0.38990 Val Amp Metric: 0.00670 Val Phi Metric: 0.41390\n",
            "Epoch: 5 Train Amp Loss: -4.40392 Train Phi Loss: -0.47879 Val Amp Loss: -4.41007 Val Phi Loss: -0.48716\n",
            "Epoch: 5 Train Amp Metric: 0.00656 Train Phi Metric: 0.36798 Val Amp Metric: 0.00654 Val Phi Metric: 0.36497\n",
            "Epoch: 6 Train Amp Loss: -4.43885 Train Phi Loss: -0.56357 Val Amp Loss: -4.41052 Val Phi Loss: -0.48285\n",
            "Epoch: 6 Train Amp Metric: 0.00644 Train Phi Metric: 0.34598 Val Amp Metric: 0.00640 Val Phi Metric: 0.35095\n",
            "Epoch: 7 Train Amp Loss: -4.45988 Train Phi Loss: -0.62050 Val Amp Loss: -4.45349 Val Phi Loss: -0.55269\n",
            "Epoch: 7 Train Amp Metric: 0.00637 Train Phi Metric: 0.33151 Val Amp Metric: 0.00639 Val Phi Metric: 0.33569\n",
            "Epoch: 8 Train Amp Loss: -4.47630 Train Phi Loss: -0.66653 Val Amp Loss: -4.27072 Val Phi Loss: -0.10157\n",
            "Epoch: 8 Train Amp Metric: 0.00631 Train Phi Metric: 0.32037 Val Amp Metric: 0.00640 Val Phi Metric: 0.36618\n",
            "Epoch: 9 Train Amp Loss: -4.49487 Train Phi Loss: -0.72165 Val Amp Loss: -4.47451 Val Phi Loss: -0.58569\n",
            "Epoch: 9 Train Amp Metric: 0.00625 Train Phi Metric: 0.30824 Val Amp Metric: 0.00629 Val Phi Metric: 0.32611\n",
            "Epoch: 10 Train Amp Loss: -4.51550 Train Phi Loss: -0.77595 Val Amp Loss: -4.49899 Val Phi Loss: -0.70524\n",
            "Epoch: 10 Train Amp Metric: 0.00619 Train Phi Metric: 0.29606 Val Amp Metric: 0.00622 Val Phi Metric: 0.30409\n",
            "Epoch: 11 Train Amp Loss: -4.52583 Train Phi Loss: -0.81004 Val Amp Loss: -4.50684 Val Phi Loss: -0.71327\n",
            "Epoch: 11 Train Amp Metric: 0.00616 Train Phi Metric: 0.28774 Val Amp Metric: 0.00619 Val Phi Metric: 0.30601\n",
            "Epoch: 12 Train Amp Loss: -4.54305 Train Phi Loss: -0.85524 Val Amp Loss: -4.52267 Val Phi Loss: -0.64724\n",
            "Epoch: 12 Train Amp Metric: 0.00611 Train Phi Metric: 0.27879 Val Amp Metric: 0.00616 Val Phi Metric: 0.32845\n",
            "Epoch: 13 Train Amp Loss: -4.54972 Train Phi Loss: -0.88366 Val Amp Loss: -4.50577 Val Phi Loss: -0.75794\n",
            "Epoch: 13 Train Amp Metric: 0.00609 Train Phi Metric: 0.27255 Val Amp Metric: 0.00615 Val Phi Metric: 0.28824\n",
            "Epoch: 14 Train Amp Loss: -4.56447 Train Phi Loss: -0.92922 Val Amp Loss: -4.54926 Val Phi Loss: -0.80216\n",
            "Epoch: 14 Train Amp Metric: 0.00605 Train Phi Metric: 0.26371 Val Amp Metric: 0.00605 Val Phi Metric: 0.28117\n",
            "Epoch: 15 Train Amp Loss: -4.57336 Train Phi Loss: -0.95694 Val Amp Loss: -4.52927 Val Phi Loss: -0.84549\n",
            "Epoch: 15 Train Amp Metric: 0.00602 Train Phi Metric: 0.25805 Val Amp Metric: 0.00610 Val Phi Metric: 0.27418\n",
            "Epoch: 16 Train Amp Loss: -4.58384 Train Phi Loss: -0.97615 Val Amp Loss: -4.56618 Val Phi Loss: -0.84857\n",
            "Epoch: 16 Train Amp Metric: 0.00599 Train Phi Metric: 0.25280 Val Amp Metric: 0.00600 Val Phi Metric: 0.27459\n",
            "Epoch: 17 Train Amp Loss: -4.58961 Train Phi Loss: -0.99620 Val Amp Loss: -4.55166 Val Phi Loss: -0.85030\n",
            "Epoch: 17 Train Amp Metric: 0.00597 Train Phi Metric: 0.24881 Val Amp Metric: 0.00602 Val Phi Metric: 0.26928\n",
            "Epoch: 18 Train Amp Loss: -4.60127 Train Phi Loss: -1.03516 Val Amp Loss: -4.57173 Val Phi Loss: -0.92916\n",
            "Epoch: 18 Train Amp Metric: 0.00594 Train Phi Metric: 0.24265 Val Amp Metric: 0.00602 Val Phi Metric: 0.25836\n",
            "Epoch: 19 Train Amp Loss: -4.60831 Train Phi Loss: -1.06121 Val Amp Loss: -4.56030 Val Phi Loss: -0.88621\n",
            "Epoch: 19 Train Amp Metric: 0.00592 Train Phi Metric: 0.23755 Val Amp Metric: 0.00598 Val Phi Metric: 0.25693\n",
            "Epoch: 20 Train Amp Loss: -4.61836 Train Phi Loss: -1.08194 Val Amp Loss: -4.55167 Val Phi Loss: -0.88540\n",
            "Epoch: 20 Train Amp Metric: 0.00589 Train Phi Metric: 0.23295 Val Amp Metric: 0.00597 Val Phi Metric: 0.25504\n",
            "Epoch: 21 Train Amp Loss: -4.62418 Train Phi Loss: -1.10293 Val Amp Loss: -4.59279 Val Phi Loss: -0.91137\n",
            "Epoch: 21 Train Amp Metric: 0.00587 Train Phi Metric: 0.22892 Val Amp Metric: 0.00597 Val Phi Metric: 0.25797\n",
            "Epoch: 22 Train Amp Loss: -4.63577 Train Phi Loss: -1.13055 Val Amp Loss: -4.58059 Val Phi Loss: -0.95171\n",
            "Epoch: 22 Train Amp Metric: 0.00584 Train Phi Metric: 0.22376 Val Amp Metric: 0.00611 Val Phi Metric: 0.26238\n",
            "Epoch: 23 Train Amp Loss: -4.63685 Train Phi Loss: -1.14140 Val Amp Loss: -4.57879 Val Phi Loss: -0.81798\n",
            "Epoch: 23 Train Amp Metric: 0.00583 Train Phi Metric: 0.22210 Val Amp Metric: 0.00602 Val Phi Metric: 0.26429\n",
            "Epoch: 24 Train Amp Loss: -4.64444 Train Phi Loss: -1.15755 Val Amp Loss: -4.54907 Val Phi Loss: -0.54932\n",
            "Epoch: 24 Train Amp Metric: 0.00581 Train Phi Metric: 0.21904 Val Amp Metric: 0.00602 Val Phi Metric: 0.27365\n",
            "Epoch: 25 Train Amp Loss: -4.65357 Train Phi Loss: -1.18789 Val Amp Loss: -4.62187 Val Phi Loss: -0.95966\n",
            "Epoch: 25 Train Amp Metric: 0.00578 Train Phi Metric: 0.21409 Val Amp Metric: 0.00590 Val Phi Metric: 0.25978\n",
            "Epoch: 26 Train Amp Loss: -4.65593 Train Phi Loss: -1.20210 Val Amp Loss: -4.53922 Val Phi Loss: -0.61023\n",
            "Epoch: 26 Train Amp Metric: 0.00577 Train Phi Metric: 0.21162 Val Amp Metric: 0.00589 Val Phi Metric: 0.25215\n",
            "Epoch: 27 Train Amp Loss: -4.66184 Train Phi Loss: -1.21550 Val Amp Loss: -4.61264 Val Phi Loss: -1.01319\n",
            "Epoch: 27 Train Amp Metric: 0.00576 Train Phi Metric: 0.20896 Val Amp Metric: 0.00585 Val Phi Metric: 0.23695\n",
            "Epoch: 28 Train Amp Loss: -4.66841 Train Phi Loss: -1.22456 Val Amp Loss: -4.62040 Val Phi Loss: -0.99195\n",
            "Epoch: 28 Train Amp Metric: 0.00573 Train Phi Metric: 0.20656 Val Amp Metric: 0.00585 Val Phi Metric: 0.24461\n",
            "Epoch: 29 Train Amp Loss: -4.67732 Train Phi Loss: -1.24201 Val Amp Loss: -4.62912 Val Phi Loss: -0.99846\n",
            "Epoch: 29 Train Amp Metric: 0.00570 Train Phi Metric: 0.20248 Val Amp Metric: 0.00582 Val Phi Metric: 0.22992\n",
            "Epoch: 30 Train Amp Loss: -4.68157 Train Phi Loss: -1.26454 Val Amp Loss: -4.62156 Val Phi Loss: -1.05897\n",
            "Epoch: 30 Train Amp Metric: 0.00569 Train Phi Metric: 0.19992 Val Amp Metric: 0.00585 Val Phi Metric: 0.23044\n",
            "Epoch: 31 Train Amp Loss: -4.68744 Train Phi Loss: -1.28478 Val Amp Loss: -4.64592 Val Phi Loss: -1.04546\n",
            "Epoch: 31 Train Amp Metric: 0.00567 Train Phi Metric: 0.19636 Val Amp Metric: 0.00583 Val Phi Metric: 0.23126\n",
            "Epoch: 32 Train Amp Loss: -4.68659 Train Phi Loss: -1.27781 Val Amp Loss: -4.62062 Val Phi Loss: -0.67377\n",
            "Epoch: 32 Train Amp Metric: 0.00567 Train Phi Metric: 0.19666 Val Amp Metric: 0.00587 Val Phi Metric: 0.25390\n",
            "Epoch: 33 Train Amp Loss: -4.69350 Train Phi Loss: -1.29451 Val Amp Loss: -4.64165 Val Phi Loss: -1.12221\n",
            "Epoch: 33 Train Amp Metric: 0.00564 Train Phi Metric: 0.19422 Val Amp Metric: 0.00581 Val Phi Metric: 0.22365\n",
            "Epoch: 34 Train Amp Loss: -4.70066 Train Phi Loss: -1.31194 Val Amp Loss: -4.66150 Val Phi Loss: -1.13221\n",
            "Epoch: 34 Train Amp Metric: 0.00562 Train Phi Metric: 0.19125 Val Amp Metric: 0.00574 Val Phi Metric: 0.21333\n",
            "Epoch: 35 Train Amp Loss: -4.70468 Train Phi Loss: -1.32944 Val Amp Loss: -4.66513 Val Phi Loss: -1.13365\n",
            "Epoch: 35 Train Amp Metric: 0.00560 Train Phi Metric: 0.18836 Val Amp Metric: 0.00569 Val Phi Metric: 0.21214\n",
            "Epoch: 36 Train Amp Loss: -4.71354 Train Phi Loss: -1.35240 Val Amp Loss: -4.65183 Val Phi Loss: -0.83007\n",
            "Epoch: 36 Train Amp Metric: 0.00557 Train Phi Metric: 0.18525 Val Amp Metric: 0.00570 Val Phi Metric: 0.23201\n",
            "Epoch: 37 Train Amp Loss: -4.71541 Train Phi Loss: -1.33032 Val Amp Loss: -4.65263 Val Phi Loss: -1.08840\n",
            "Epoch: 37 Train Amp Metric: 0.00556 Train Phi Metric: 0.18621 Val Amp Metric: 0.00570 Val Phi Metric: 0.21795\n",
            "Epoch: 38 Train Amp Loss: -4.72040 Train Phi Loss: -1.36834 Val Amp Loss: -4.63570 Val Phi Loss: -0.81953\n",
            "Epoch: 38 Train Amp Metric: 0.00554 Train Phi Metric: 0.18266 Val Amp Metric: 0.00568 Val Phi Metric: 0.22385\n",
            "Epoch: 39 Train Amp Loss: -4.72507 Train Phi Loss: -1.37728 Val Amp Loss: -4.60972 Val Phi Loss: -0.52963\n",
            "Epoch: 39 Train Amp Metric: 0.00552 Train Phi Metric: 0.18052 Val Amp Metric: 0.00568 Val Phi Metric: 0.23104\n",
            "Epoch: 40 Train Amp Loss: -4.73026 Train Phi Loss: -1.38205 Val Amp Loss: -4.63872 Val Phi Loss: -0.94012\n",
            "Epoch: 40 Train Amp Metric: 0.00550 Train Phi Metric: 0.17866 Val Amp Metric: 0.00562 Val Phi Metric: 0.21137\n",
            "Epoch: 41 Train Amp Loss: -4.72824 Train Phi Loss: -1.37904 Val Amp Loss: -4.58747 Val Phi Loss: -0.73196\n",
            "Epoch: 41 Train Amp Metric: 0.00550 Train Phi Metric: 0.17936 Val Amp Metric: 0.00568 Val Phi Metric: 0.22610\n",
            "Epoch: 42 Train Amp Loss: -4.73706 Train Phi Loss: -1.39292 Val Amp Loss: -4.67418 Val Phi Loss: -1.20851\n",
            "Epoch: 42 Train Amp Metric: 0.00547 Train Phi Metric: 0.17684 Val Amp Metric: 0.00563 Val Phi Metric: 0.20380\n",
            "Epoch: 43 Train Amp Loss: -4.74407 Train Phi Loss: -1.40156 Val Amp Loss: -4.69423 Val Phi Loss: -1.21614\n",
            "Epoch: 43 Train Amp Metric: 0.00544 Train Phi Metric: 0.17412 Val Amp Metric: 0.00556 Val Phi Metric: 0.19934\n",
            "Epoch: 44 Train Amp Loss: -4.74647 Train Phi Loss: -1.41168 Val Amp Loss: -4.70364 Val Phi Loss: -1.08466\n",
            "Epoch: 44 Train Amp Metric: 0.00543 Train Phi Metric: 0.17314 Val Amp Metric: 0.00559 Val Phi Metric: 0.21149\n",
            "Epoch: 45 Train Amp Loss: -4.74693 Train Phi Loss: -1.42862 Val Amp Loss: -4.62936 Val Phi Loss: -0.97114\n",
            "Epoch: 45 Train Amp Metric: 0.00542 Train Phi Metric: 0.17172 Val Amp Metric: 0.00560 Val Phi Metric: 0.21373\n",
            "Epoch: 46 Train Amp Loss: -4.74918 Train Phi Loss: -1.40932 Val Amp Loss: -4.67566 Val Phi Loss: -1.09979\n",
            "Epoch: 46 Train Amp Metric: 0.00540 Train Phi Metric: 0.17269 Val Amp Metric: 0.00556 Val Phi Metric: 0.20563\n",
            "Epoch: 47 Train Amp Loss: -4.76105 Train Phi Loss: -1.44020 Val Amp Loss: -4.67675 Val Phi Loss: -1.18604\n",
            "Epoch: 47 Train Amp Metric: 0.00536 Train Phi Metric: 0.16819 Val Amp Metric: 0.00556 Val Phi Metric: 0.19986\n",
            "Epoch: 48 Train Amp Loss: -4.76151 Train Phi Loss: -1.44712 Val Amp Loss: -4.69526 Val Phi Loss: -0.95156\n",
            "Epoch: 48 Train Amp Metric: 0.00535 Train Phi Metric: 0.16813 Val Amp Metric: 0.00551 Val Phi Metric: 0.20399\n",
            "Epoch: 49 Train Amp Loss: -4.76743 Train Phi Loss: -1.46420 Val Amp Loss: -4.70526 Val Phi Loss: -1.27912\n",
            "Epoch: 49 Train Amp Metric: 0.00532 Train Phi Metric: 0.16537 Val Amp Metric: 0.00556 Val Phi Metric: 0.19306\n",
            "Epoch: 50 Train Amp Loss: -4.76569 Train Phi Loss: -1.46593 Val Amp Loss: -4.68715 Val Phi Loss: -1.13070\n",
            "Epoch: 50 Train Amp Metric: 0.00532 Train Phi Metric: 0.16486 Val Amp Metric: 0.00551 Val Phi Metric: 0.20098\n",
            "Epoch: 51 Train Amp Loss: -4.77613 Train Phi Loss: -1.46443 Val Amp Loss: -4.72826 Val Phi Loss: -1.32204\n",
            "Epoch: 51 Train Amp Metric: 0.00528 Train Phi Metric: 0.16360 Val Amp Metric: 0.00547 Val Phi Metric: 0.18887\n",
            "Epoch: 52 Train Amp Loss: -4.77779 Train Phi Loss: -1.46115 Val Amp Loss: -4.69748 Val Phi Loss: -1.21785\n",
            "Epoch: 52 Train Amp Metric: 0.00527 Train Phi Metric: 0.16353 Val Amp Metric: 0.00548 Val Phi Metric: 0.20058\n",
            "Epoch: 53 Train Amp Loss: -4.77812 Train Phi Loss: -1.47734 Val Amp Loss: -4.67399 Val Phi Loss: -1.13992\n",
            "Epoch: 53 Train Amp Metric: 0.00526 Train Phi Metric: 0.16256 Val Amp Metric: 0.00547 Val Phi Metric: 0.19937\n",
            "Epoch: 54 Train Amp Loss: -4.78339 Train Phi Loss: -1.46830 Val Amp Loss: -4.66769 Val Phi Loss: -0.95331\n",
            "Epoch: 54 Train Amp Metric: 0.00523 Train Phi Metric: 0.16175 Val Amp Metric: 0.00550 Val Phi Metric: 0.20630\n",
            "Epoch: 55 Train Amp Loss: -4.78603 Train Phi Loss: -1.48800 Val Amp Loss: -4.71755 Val Phi Loss: -1.17533\n",
            "Epoch: 55 Train Amp Metric: 0.00522 Train Phi Metric: 0.16117 Val Amp Metric: 0.00543 Val Phi Metric: 0.19163\n",
            "Epoch: 56 Train Amp Loss: -4.79157 Train Phi Loss: -1.49155 Val Amp Loss: -4.73969 Val Phi Loss: -1.22242\n",
            "Epoch: 56 Train Amp Metric: 0.00519 Train Phi Metric: 0.15902 Val Amp Metric: 0.00539 Val Phi Metric: 0.18928\n",
            "Epoch: 57 Train Amp Loss: -4.79827 Train Phi Loss: -1.50515 Val Amp Loss: -4.66461 Val Phi Loss: -1.13665\n",
            "Epoch: 57 Train Amp Metric: 0.00516 Train Phi Metric: 0.15725 Val Amp Metric: 0.00539 Val Phi Metric: 0.18946\n",
            "Epoch: 58 Train Amp Loss: -4.79893 Train Phi Loss: -1.51254 Val Amp Loss: -4.70066 Val Phi Loss: -1.21260\n",
            "Epoch: 58 Train Amp Metric: 0.00516 Train Phi Metric: 0.15734 Val Amp Metric: 0.00558 Val Phi Metric: 0.20098\n",
            "Epoch: 59 Train Amp Loss: -4.80061 Train Phi Loss: -1.50757 Val Amp Loss: -4.68738 Val Phi Loss: -0.92129\n",
            "Epoch: 59 Train Amp Metric: 0.00514 Train Phi Metric: 0.15694 Val Amp Metric: 0.00549 Val Phi Metric: 0.21706\n",
            "Epoch: 60 Train Amp Loss: -4.80445 Train Phi Loss: -1.51739 Val Amp Loss: -4.74314 Val Phi Loss: -1.21699\n",
            "Epoch: 60 Train Amp Metric: 0.00512 Train Phi Metric: 0.15536 Val Amp Metric: 0.00531 Val Phi Metric: 0.18019\n",
            "Epoch: 61 Train Amp Loss: -4.81329 Train Phi Loss: -1.54388 Val Amp Loss: -4.73750 Val Phi Loss: -0.91249\n",
            "Epoch: 61 Train Amp Metric: 0.00509 Train Phi Metric: 0.15231 Val Amp Metric: 0.00539 Val Phi Metric: 0.20680\n",
            "Epoch: 62 Train Amp Loss: -4.81241 Train Phi Loss: -1.54490 Val Amp Loss: -4.75224 Val Phi Loss: -1.16154\n",
            "Epoch: 62 Train Amp Metric: 0.00508 Train Phi Metric: 0.15220 Val Amp Metric: 0.00529 Val Phi Metric: 0.18698\n",
            "Epoch: 63 Train Amp Loss: -4.81843 Train Phi Loss: -1.54950 Val Amp Loss: -4.75125 Val Phi Loss: -1.30416\n",
            "Epoch: 63 Train Amp Metric: 0.00506 Train Phi Metric: 0.15193 Val Amp Metric: 0.00529 Val Phi Metric: 0.18270\n",
            "Epoch: 64 Train Amp Loss: -4.82321 Train Phi Loss: -1.55187 Val Amp Loss: -4.73365 Val Phi Loss: -1.11106\n",
            "Epoch: 64 Train Amp Metric: 0.00504 Train Phi Metric: 0.15062 Val Amp Metric: 0.00537 Val Phi Metric: 0.20265\n",
            "Epoch: 65 Train Amp Loss: -4.82249 Train Phi Loss: -1.54115 Val Amp Loss: -4.73521 Val Phi Loss: -1.21110\n",
            "Epoch: 65 Train Amp Metric: 0.00503 Train Phi Metric: 0.15116 Val Amp Metric: 0.00532 Val Phi Metric: 0.18519\n",
            "Epoch: 66 Train Amp Loss: -4.82363 Train Phi Loss: -1.55091 Val Amp Loss: -4.72223 Val Phi Loss: -1.12140\n",
            "Epoch: 66 Train Amp Metric: 0.00502 Train Phi Metric: 0.15127 Val Amp Metric: 0.00533 Val Phi Metric: 0.18891\n",
            "Epoch: 67 Train Amp Loss: -4.82919 Train Phi Loss: -1.57421 Val Amp Loss: -4.74683 Val Phi Loss: -1.30682\n",
            "Epoch: 67 Train Amp Metric: 0.00499 Train Phi Metric: 0.14842 Val Amp Metric: 0.00531 Val Phi Metric: 0.18757\n",
            "Epoch: 68 Train Amp Loss: -4.83673 Train Phi Loss: -1.57033 Val Amp Loss: -4.73353 Val Phi Loss: -1.28313\n",
            "Epoch: 68 Train Amp Metric: 0.00497 Train Phi Metric: 0.14785 Val Amp Metric: 0.00522 Val Phi Metric: 0.17661\n",
            "Epoch: 69 Train Amp Loss: -4.84019 Train Phi Loss: -1.58063 Val Amp Loss: -4.75791 Val Phi Loss: -1.16866\n",
            "Epoch: 69 Train Amp Metric: 0.00494 Train Phi Metric: 0.14639 Val Amp Metric: 0.00527 Val Phi Metric: 0.18807\n",
            "Epoch: 70 Train Amp Loss: -4.83898 Train Phi Loss: -1.59006 Val Amp Loss: -4.76128 Val Phi Loss: -1.36222\n",
            "Epoch: 70 Train Amp Metric: 0.00494 Train Phi Metric: 0.14640 Val Amp Metric: 0.00530 Val Phi Metric: 0.18170\n",
            "Epoch: 71 Train Amp Loss: -4.84611 Train Phi Loss: -1.59312 Val Amp Loss: -4.72360 Val Phi Loss: -0.98086\n",
            "Epoch: 71 Train Amp Metric: 0.00492 Train Phi Metric: 0.14501 Val Amp Metric: 0.00531 Val Phi Metric: 0.20067\n",
            "Epoch: 72 Train Amp Loss: -4.84839 Train Phi Loss: -1.58340 Val Amp Loss: -4.74282 Val Phi Loss: -0.96880\n",
            "Epoch: 72 Train Amp Metric: 0.00491 Train Phi Metric: 0.14501 Val Amp Metric: 0.00528 Val Phi Metric: 0.19720\n",
            "Epoch: 73 Train Amp Loss: -4.85293 Train Phi Loss: -1.59977 Val Amp Loss: -4.76939 Val Phi Loss: -1.32169\n",
            "Epoch: 73 Train Amp Metric: 0.00489 Train Phi Metric: 0.14429 Val Amp Metric: 0.00521 Val Phi Metric: 0.17458\n",
            "Epoch: 74 Train Amp Loss: -4.85305 Train Phi Loss: -1.60645 Val Amp Loss: -4.77142 Val Phi Loss: -1.36962\n",
            "Epoch: 74 Train Amp Metric: 0.00487 Train Phi Metric: 0.14364 Val Amp Metric: 0.00517 Val Phi Metric: 0.17375\n",
            "Epoch: 75 Train Amp Loss: -4.85607 Train Phi Loss: -1.58928 Val Amp Loss: -4.76014 Val Phi Loss: -1.35812\n",
            "Epoch: 75 Train Amp Metric: 0.00486 Train Phi Metric: 0.14402 Val Amp Metric: 0.00512 Val Phi Metric: 0.17206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U--Q5chr1qk2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}